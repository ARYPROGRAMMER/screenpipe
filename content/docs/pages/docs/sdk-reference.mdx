import MotionDiv from '../../components/motion-div'
import { Tabs, Tab } from 'nextra/components'

# javascript sdk reference

<MotionDiv>

screenpipe provides two sdk packages:
- `@screenpipe/js` - for node.js environments (nextjs api routes, etc)
- `@screenpipe/browser` - for browser environments

both sdks provide type-safe interfaces to interact with screenpipe's core functionality.

</MotionDiv>

<MotionDiv delay={0.3}>

### installation

#### node.js sdk

<Tabs items={['npm', 'pnpm', 'bun', 'yarn']}>
  <Tab>
    ```bash copy
    npm install @screenpipe/js
    ```
  </Tab>
  <Tab>
    ```bash copy
    pnpm add @screenpipe/js
    ```
  </Tab>
  <Tab>
    ```bash copy
    bun add @screenpipe/js
    ```
  </Tab>
  <Tab>
    ```bash copy
    yarn add @screenpipe/js
    ```
  </Tab>
</Tabs>

#### browser sdk

<Tabs items={['npm', 'pnpm', 'bun', 'yarn']}>
  <Tab>
    ```bash copy
    npm install @screenpipe/browser
    ```
  </Tab>
  <Tab>
    ```bash copy
    pnpm add @screenpipe/browser
    ```
  </Tab>
  <Tab>
    ```bash copy
    bun add @screenpipe/browser
    ```
  </Tab>
  <Tab>
    ```bash copy
    yarn add @screenpipe/browser
    ```
  </Tab>
</Tabs>

### basic usage

```typescript
// node.js
import { pipe } from '@screenpipe/js'

// browser
import { pipe } from '@screenpipe/browser'
```

### search api

```typescript
const results = await pipe.queryScreenpipe({
  q: "meeting notes",
  contentType: "ocr", // "ocr" | "audio" | "ui" | "all" | "audio+ui" | "ocr+ui" | "audio+ocr"
  limit: 10,
  offset: 0,
  startTime: "2024-03-10T12:00:00Z",
  endTime: "2024-03-10T13:00:00Z",
  appName: "chrome",
  windowName: "meeting",
  includeFrames: true,
  minLength: 10,
  maxLength: 1000,
  speakerIds: [1, 2],
  frameName: "screenshot.png"
})
```

### common usage patterns

#### fetching recent screen activity

```typescript
// Get the last hour of screen activity from Chrome
const oneHourAgo = new Date(Date.now() - 60 * 60 * 1000).toISOString()
const now = new Date().toISOString()

const results = await pipe.queryScreenpipe({
  contentType: "ocr",
  startTime: oneHourAgo,
  endTime: now,
  appName: "chrome",
  limit: 50,
  includeFrames: true, // include base64 encoded images
})

// Process results
for (const item of results.data) {
  console.log(`At ${item.content.timestamp}:`)
  console.log(`Text: ${item.content.text}`)
  
  if (item.content.frame) {
    // You could display or process the base64 image
    // e.g., <img src={`data:image/png;base64,${item.content.frame}`} />
  }
}
```

#### searching for specific content

```typescript
// Search for "meeting notes" in all applications
const results = await pipe.queryScreenpipe({
  q: "meeting notes",
  contentType: "all", // search across ocr, audio and ui
  limit: 20,
})

// Process results based on content type
for (const item of results.data) {
  if (item.type === "OCR") {
    console.log(`Screen text: ${item.content.text}`)
  } else if (item.type === "Audio") {
    console.log(`Audio transcript: ${item.content.transcription}`)
    console.log(`Speaker: ${item.content.speaker_id}`)
  } else if (item.type === "UI") {
    console.log(`UI element: ${item.content.element_type}`)
  }
}
```

#### building a timeline of user activity

```typescript
// Get a full day of activity
const startOfDay = new Date()
startOfDay.setHours(0, 0, 0, 0)

const endOfDay = new Date() 
endOfDay.setHours(23, 59, 59, 999)

// Get both screen and audio data
const results = await pipe.queryScreenpipe({
  contentType: "ocr+audio",
  startTime: startOfDay.toISOString(),
  endTime: endOfDay.toISOString(),
  limit: 1000, // Get a large sample
})

// Group by hour for timeline visualization
const timelineByHour = {}

for (const item of results.data) {
  const timestamp = new Date(item.content.timestamp)
  const hour = timestamp.getHours()
  
  // Initialize hour bucket if needed
  if (!timelineByHour[hour]) {
    timelineByHour[hour] = {
      screenEvents: 0,
      audioEvents: 0,
      apps: new Set(),
    }
  }
  
  // Update counts
  if (item.type === "OCR") {
    timelineByHour[hour].screenEvents++
    
    if (item.content.app_name) {
      timelineByHour[hour].apps.add(item.content.app_name)
    }
  } else if (item.type === "Audio") {
    timelineByHour[hour].audioEvents++
  }
}

// Now timelineByHour can be used to build a visualization
console.log(timelineByHour)
```

### input control api

requires building the backend with the `experimental` feature flag.

`cargo build --release --features experimental`

```typescript
// type text
await pipe.input.type("hello world")

// press key
await pipe.input.press("enter")

// move mouse
await pipe.input.moveMouse(100, 200)

// click
await pipe.input.click("left") // "left" | "right" | "middle"
```

#### automating repetitive tasks

```typescript
// Example: Automate filling a form
async function fillForm() {
  // Move to input field and click
  await pipe.input.moveMouse(300, 200)
  await pipe.input.click("left")
  
  // Type name
  await pipe.input.type("John Doe")
  
  // Tab to next field
  await pipe.input.press("tab")
  
  // Type email
  await pipe.input.type("john@example.com")
  
  // Tab to next field 
  await pipe.input.press("tab")
  
  // Type message
  await pipe.input.type("This is an automated message")
  
  // Submit form
  await pipe.input.press("tab")
  await pipe.input.press("enter")
}

// Call the function
await fillForm()
```

### realtime streams

```typescript
// stream transcriptions
for await (const chunk of pipe.streamTranscriptions()) {
  console.log(chunk.choices[0].text)
  console.log(chunk.metadata) // { timestamp, device, isInput }
}

// stream vision events
for await (const event of pipe.streamVision(true)) { // true to include images
  console.log(event.data.text)
  console.log(event.data.app_name)
  console.log(event.data.image) // base64 if includeImages=true
}
```

#### real-time meeting summarizer

```typescript
import OpenAI from 'openai'

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
})

async function summarizeMeeting() {
  let transcript = ""
  let summaryBuffer = ""
  let lastSummaryTime = Date.now()
  
  // Stream transcriptions in real-time
  for await (const chunk of pipe.streamTranscriptions()) {
    const text = chunk.choices[0].text
    transcript += text + " "
    
    // Generate summary every 30 seconds
    if (Date.now() - lastSummaryTime > 30000 && transcript.length > 200) {
      const summary = await openai.chat.completions.create({
        model: "gpt-3.5-turbo",
        messages: [
          {
            role: "system",
            content: "You summarize meeting transcripts concisely."
          },
          {
            role: "user",
            content: `Summarize this meeting transcript: ${transcript}`
          }
        ]
      })
      
      summaryBuffer = summary.choices[0].message.content
      lastSummaryTime = Date.now()
      
      // Display or store the summary
      console.log("Meeting Summary Update:", summaryBuffer)
    }
  }
}

// Start summarizing
summarizeMeeting().catch(console.error)
```

#### real-time screen activity monitor

```typescript
async function monitorScreenActivity() {
  // Monitor screen content in real-time
  for await (const event of pipe.streamVision(true)) {
    // Check if we're looking at important apps
    const appName = event.data.app_name?.toLowerCase() || ""
    const text = event.data.text?.toLowerCase() || ""
    
    // Example: Detect when looking at email
    if (
      appName.includes("outlook") || 
      appName.includes("gmail") ||
      text.includes("inbox") ||
      text.includes("compose email")
    ) {
      console.log("Email activity detected at", new Date().toLocaleTimeString())
      
      // You could log this, send notifications, etc.
    }
    
    // Example: Track time spent in different applications
    // This would require keeping state between events
  }
}

// Start monitoring
monitorScreenActivity().catch(console.error)
```

### notifications (desktop)

```typescript
await pipe.sendDesktopNotification({
  title: "meeting starting",
  body: "your standup begins in 5 minutes",
})
```

### node.js specific features

the node sdk includes additional features not available in the browser:

```typescript
// settings management
const settings = await pipe.settings.getAll()
await pipe.settings.update({ aiModel: "gpt-4" })

// inbox management (node only)
const messages = await pipe.inbox.getMessages()
await pipe.inbox.clearMessages()
```

### typescript types

both sdks export comprehensive typescript types:

```typescript
import type {
  ContentType,
  ScreenpipeQueryParams,
  ScreenpipeResponse,
  OCRContent,
  AudioContent,
  UiContent,
  Speaker,
  NotificationOptions,
  Settings,
  // ... and more
} from '@screenpipe/js' // or '@screenpipe/browser'
```

key types include:

```typescript
type ContentType = "all" | "ocr" | "audio" | "ui" | "audio+ui" | "ocr+ui" | "audio+ocr"

interface ScreenpipeQueryParams {
  q?: string
  contentType?: ContentType
  limit?: number
  offset?: number
  startTime?: string
  endTime?: string
  appName?: string
  windowName?: string
  includeFrames?: boolean
  minLength?: number
  maxLength?: number
  speakerIds?: number[]
  frameName?: string
}

interface ScreenpipeResponse {
  data: ContentItem[] // OCR | Audio | UI content
  pagination: {
    limit: number
    offset: number
    total: number
  }
}
```

### error handling

```typescript
try {
  const results = await pipe.queryScreenpipe({
    q: "meeting",
    contentType: "ocr"
  })
} catch (error) {
  console.error("screenpipe api error:", error)
}
```

### integrating with llms

```typescript
import OpenAI from 'openai'
import { pipe } from '@screenpipe/js'

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
})

async function getContextAndQueryLLM(query) {
  // First, get relevant context from screenpipe
  const oneHourAgo = new Date(Date.now() - 60 * 60 * 1000).toISOString()
  const now = new Date().toISOString()
  
  const results = await pipe.queryScreenpipe({
    q: query, // Use the user's query to find relevant content
    contentType: "all",
    startTime: oneHourAgo,
    endTime: now,
    limit: 20,
  })
  
  // Extract text from results
  let contextText = ""
  for (const item of results.data) {
    if (item.type === "OCR") {
      contextText += `[Screen Text]: ${item.content.text}\n`
    } else if (item.type === "Audio") {
      contextText += `[Audio Transcript]: ${item.content.transcription}\n`
    }
  }
  
  // Now query the LLM with context
  const llmResponse = await openai.chat.completions.create({
    model: "gpt-4",
    messages: [
      {
        role: "system",
        content: "You are a helpful assistant that answers questions based on the user's recent activity."
      },
      {
        role: "user",
        content: `Here is context from my recent computer activity:\n\n${contextText}\n\nNow, answer this question: ${query}`
      }
    ]
  })
  
  return llmResponse.choices[0].message.content
}

// Example usage
const answer = await getContextAndQueryLLM("What was that meeting about?")
console.log(answer)
```

### examples

check out our [production pipe examples](https://github.com/mediar-ai/screenpipe/tree/main/pipes) to see real-world usage of the sdk:

- data visualization pipe
- linkedin ai assistant
- meeting summarizer
- memories gallery
- obsidian integration
- search interface

these examples demonstrate best practices and common patterns when building with screenpipe's sdk.

</MotionDiv>


### settings management

<MotionDiv delay={0.7}>

pipes can access and modify screenpipe app settings through the SDK. this is useful for storing pipe-specific configuration and accessing global app settings.

#### quick start with CLI

the fastest way to add settings management to your pipe is using our CLI:

```bash copy
bunx --bun @screenpipe/dev@latest components add
# select "use-pipe-settings" from the menu
```

this will add the following components to your pipe:
- `use-pipe-settings` hook for react components
- `get-screenpipe-app-settings` server action
- required typescript types

#### manual setup

1. create types for your settings:

```typescript
// src/lib/types.ts
import type { Settings as ScreenpipeAppSettings } from '@screenpipe/js'

export interface Settings {
  // your pipe specific settings
  customSetting?: string
  anotherSetting?: number
  
  // screenpipe app settings
  screenpipeAppSettings?: ScreenpipeAppSettings
}
```

2. create server action to access settings:

```typescript
// src/lib/actions/get-screenpipe-app-settings.ts
import { pipe } from '@screenpipe/js'
import type { Settings as ScreenpipeAppSettings } from '@screenpipe/js'

export async function getScreenpipeAppSettings() {
  return await pipe.settings.getAll()
}

export async function updateScreenpipeAppSettings(
  newSettings: Partial<ScreenpipeAppSettings>
) {
  return await pipe.settings.update(newSettings)
}
```

3. create react hook for settings management:

```typescript
// src/lib/hooks/use-pipe-settings.tsx
import { useState, useEffect } from 'react'
import { Settings } from '@/lib/types'
import {
  getScreenpipeAppSettings,
  updateScreenpipeAppSettings,
} from '@/lib/actions/get-screenpipe-app-settings'

const DEFAULT_SETTINGS: Partial<Settings> = {
  customSetting: 'default value',
  anotherSetting: 42,
}

export function usePipeSettings() {
  const [settings, setSettings] = useState<Partial<Settings> | null>(null)
  const [loading, setLoading] = useState(true)

  useEffect(() => {
    loadSettings()
  }, [])

  const loadSettings = async () => {
    try {
      // load screenpipe app settings
      const screenpipeSettings = await getScreenpipeAppSettings()

      // get pipe specific settings from customSettings
      const pipeSettings = {
        ...(screenpipeSettings.customSettings?.yourPipeName && {
          ...screenpipeSettings.customSettings.yourPipeName,
        }),
      }

      // merge everything together
      setSettings({
        ...DEFAULT_SETTINGS,
        ...pipeSettings,
        screenpipeAppSettings: screenpipeSettings,
      })
    } catch (error) {
      console.error('failed to load settings:', error)
    } finally {
      setLoading(false)
    }
  }

  const updateSettings = async (newSettings: Partial<Settings>) => {
    try {
      // split settings
      const { screenpipeAppSettings, ...pipeSettings } = newSettings

      const mergedPipeSettings = {
        ...DEFAULT_SETTINGS,
        ...pipeSettings,
      }

      // update screenpipe settings if provided
      await updateScreenpipeAppSettings({
        ...screenpipeAppSettings,
        customSettings: {
          ...screenpipeAppSettings?.customSettings,
          yourPipeName: pipeSettings,
        },
      })

      // update state with everything
      setSettings({
        ...mergedPipeSettings,
        screenpipeAppSettings:
          screenpipeAppSettings || settings?.screenpipeAppSettings,
      })
      return true
    } catch (error) {
      console.error('failed to update settings:', error)
      return false
    }
  }

  return { settings, updateSettings, loading }
}
```

4. use in your components:

```typescript
import { usePipeSettings } from '@/lib/hooks/use-pipe-settings'

export function SettingsComponent() {
  const { settings, updateSettings, loading } = usePipeSettings()

  if (loading) return <div>loading...</div>

  return (
    <form onSubmit={async (e) => {
      e.preventDefault()
      const formData = new FormData(e.target as HTMLFormElement)
      await updateSettings({
        customSetting: formData.get('customSetting') as string,
        anotherSetting: parseInt(formData.get('anotherSetting') as string),
      })
    }}>
      <input 
        name="customSetting"
        defaultValue={settings?.customSetting}
      />
      <input 
        name="anotherSetting"
        type="number"
        defaultValue={settings?.anotherSetting}
      />
      <button type="submit">save</button>
    </form>
  )
}
```

#### best practices

- store pipe-specific settings under `customSettings.yourPipeName` in screenpipe app settings
- use typescript for type safety
- provide default values for all settings
- handle loading and error states
- validate settings before saving
- use server actions for settings operations
- consider using shadcn/ui components for consistent UI

see the [obsidian pipe](https://github.com/mediar-ai/screenpipe/tree/main/pipes/obsidian) for a complete example of settings management.

</MotionDiv>

### LLM links 

paste these links into your Cursor chat for context:

- https://github.com/mediar-ai/screenpipe/blob/main/screenpipe-js/browser-sdk/src/index.ts
- https://github.com/mediar-ai/screenpipe/blob/main/screenpipe-js/node-sdk/src/index.ts

